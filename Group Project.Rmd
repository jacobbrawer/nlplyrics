---
title: "Popular Song Lyric Analysis from 1970-2019"
author: "Will Abele, Jacob Brawer, Owen Rosebeck"
date: 'Wednesday, December 18'
output:
  html_document:
    toc: true
    number_sections: true
  pdf_document: default
---

https://prezi.com/view/zYGal9DWEFulQ9FjeTOM/

```{r setup, include=FALSE, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
songs2013 <- read.csv("songs2013.csv")
songs2013[songs2013 == "null"] <- NA
songs2013 <- na.omit(songs2013)

```



# Introduction

One relaxing Friday evening several weeks ago, the three of us sat around a circle table in Frary Dining Hall enjoying a long, pleasant dinner filled with scintillating topics of conversation. As we talked, we eventually began discussing our interests in music, and specifically, which era of music was best. Owen appreciated the ‘80s, Will the 00’s, and Jacob the ‘10s, each for different reasons. Owen claimed the ‘80s was a “fun” and “happy” era of music, partially as a result of its minimal political overtones. Will agreed with Owen’s point, but noted the wide variety of genres in the 2000’s that had not been previously prevalent. Jacob argued that music in the 2010’s has featured the most creativity and experimentation. As any Pomona students would, we determined this Frary argument should be examined under the scope of academic, data-backed research, and our Computational Statistics semester project proved the perfect outlet.

As shown in the charts below, Owen may be correct about the 80's, especially when compared to the 10's!

```{r, echo=FALSE, message=FALSE}
library(tm)
library(tidyverse)
songs <- read.csv("prepped_data.csv")
songs <- rbind(songs, songs2013)

stopWords <- c("chorus ", "verse ", "intro ", "1 ", "bridge ", "2 ", "Chorus ", "Intro ", "Verse ", "Bridge ")


songs <- songs %>% 
  select(Album, Artist, Lyrics, Rank, Song.Title, Year, Verbs, Nouns, Adverbs, Word.Counts, Corpus) %>%
  mutate(Lyrics = as.character(Lyrics)) %>% 
  mutate(Verbs = as.character(Verbs)) %>% 
  mutate(Nouns = as.character(Nouns)) %>% 
  mutate(Adverbs = as.character(Adverbs)) %>% 
  mutate(Nouns = removeWords(Nouns, stopWords)) %>%
  mutate(Decade = ifelse(Year < 1980, 1970, ifelse(Year < 1990, 1980, ifelse(Year < 2000, 1990, ifelse(Year < 2010, 2000, 2010))))) %>% 
  mutate(Corpus = removeWords(tolower(as.character(Corpus)), stopWords)) 

```

```{r, echo=FALSE, message=FALSE}
# these two figures are for "hook" intro only; will see later

library(dplyr)
library(tidyr) 
library(widyr)
library(ggplot2)
library(ggrepel) 
library(gridExtra) 
library(knitr) 
library(kableExtra) 
library(formattable)
library(circlize) 
library(tidytext) 
library(textdata)
library(radarchart)

afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
nrc <- get_sentiments("nrc")

songs80 <- songs %>%
  filter(Decade == 1980)

songs10 <- songs %>%
  filter(Decade == 2010)

songtoken80 <- songs80 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy80 <- songtoken80 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

songtoken10 <- songs10 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy10 <- songtoken10 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

song80_nrc <- song_tidy80 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_80s <- song80_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "1980s Sentiment")

radar_chart_80s

song10_nrc <- song_tidy10 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_10s <- song10_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "2010s Sentiment")

radar_chart_10s
```


# Project Goals and General Methods

Of course, music preferences are subjective, and thus it is impossible to determine the “best” era of music. That said, characterizing a time period’s ideas and sentiments can be done fairly objectively through analyzing that time period’s music, and more specifically, the language and lyrics used in the songs of each era. Song lyrics are a key identifier when it comes to determining song popularity (Berger, 2018), and they provide an opportunity for rich analyses of their associated time period. 

Hence, our group’s goal is to analyze (popular) song lyrics in the yearly Billboard Top 100 charts from 1970 to 2019 to determine and visualize word, concept, and sentiment popularity, as well as the dynamics of each. As discussed in the Data section below, the majority of our data is textual. Thus, many of the methods we use in our analyses are branches and applications of Natural Language Processing (NLP).  More specifically, we used computational NLP techniques in order to examine word popularity, lyrical sentiment, and word uniqueness, in addition to an even larger array of techniques detailed below across the last five decades. 

In order to allocate, clean, wrangle, merge, and visualize the data, we used a variety of different packages in both Python (regular expressions, BeautifulSoup, requests, spacy, etc.) and R (tidytext, tidyr, dplyr, ggplot, wordcloud2, etc.) and heavily relied on the python based Genius API.


# Data

We originally had planned to use two datasets. One was from DataWorlds, provided by Sean Miller, and featured 50,000 Billboard top 100 songs, compiled every week from 1960-2019. The other was the Metrolyrics’s 300,000+ song lyrics dataset from kaggle. We wanted to merge these data sets by song, which proved to be a rather daunting task, due to a number of factors. Despite 300,000 songs being a large amount, there was not nearly as much overlap with the 50,000 Billboard songs as we expected. Additionally, there were a number of string-related differences that would’ve been very time consuming to standardize, stemming from differences in handling featured artists (sometimes in the Artist’s column, sometimes in the Song Title column, and with different characters to demonstrate connection such as “feat., ft., x, &, featuring” and even both artists names listed without any formal union. 
 
We opted to search for a second plan after dabbling with regular expressions enough to realize this was not going to be fruitful. Enter Genius API. We discovered through some in-depth research examining precedent work that Genius is very generous in letting amateur data scientists access their API, which enabled us to make thousands and thousands of calls for free without being throttled (for the most part).  Using the Python-based API, we were able to iterate through our 50,000 Billboard song data-base, which we split up by decade and scraped song-lyrics from Genius’s website for each song. This was no small undertaking. It took, on average, about 3-5 hours for each decade to scrape. We would run these overnight, and eventually, after several days, we had collected all of our lyrical data. Using some basic cleaning procedures in R, we removed rows without lyrics and began preliminary analyses by generating basic word clouds. We noticed the appearance of several unusual characters (â, €, etc.). Despite toying with regular expressions in both R and Python, we were unable to clean the lyrics to the degree that we wanted. 

We decided to ultimately pivot from this original weekly Billboard song dataset, citing the lesser known songs in the database’s compatibility issues with the Genius API. Our third and final plan involved starting from scratch. We decided to only look at yearly (instead of weekly) Billboard Top 100 data from 1970-2019. To do this, we began by using the requests and BeautifulSoup packages in Python to build a scraper that would output a dataframe including, song title, position and year for our range of decades. We pulled this data from billboards.com and billboardtop100of.com. We then iterated through this data-frame, once again using the Genius API, to compile lyrics for songs. However, again we noticed some unusual characters in some of our song lyrics - some of the “lyrics” were not lyrics! If the Genius API could not find the exact song, it would pick, at random, another page on its website and scrape that information instead. (In one case, we pulled an entire episode of Bojack Horseman instead of the intended song because of punctuation in the song title.) We implemented a fix that checked if the title of the webpage was equal to the title of the song in order to fix this problem. 

After all this work, our final data-set was complete! A data-frame including Song Title, Artist, Song Position on Billboard, Lyrics, Year, Featured Artists and the link to the page from which we pulled each song. Because our data files were so enormous, we hosted our data locally. Pictured below is what some of our final data frame looked like.

![Final Data Frame](data.png)


# Procedure

After completing our dataset, next steps involved pre-processing our lyrics for easier manipulation in R and inputting our natural language processing pipeline. The Python code for the process below can be found in the appendix. 

We began using the spacy package in Python on the raw lyrics-- this process involved removing all punctuation, and utilizing regular expressions to make our lyrics alphanumeric and lowercase. Once this was done, we were able to use the spacy package’s core NLP cleaning functionalities with some online help from previous natural language processing projects that involved song lyrics to build a function that would take in our alphanumeric/lowercase lyrics and output a string that was the corpus, or main body of words, within each set of lyrics, excluding articles and other “Stop Words,” which were taken from an online dictionary and appended to our our own list of words that we didn’t want to include such as “Verse” and “Chorus.” This corpus was then parsed through in order to create separate strings for verbs, nouns and adverbs (using a processed referred to as lemmatization)  and a value for word count within each corpus. These were all appended as columns to the original data set, which was then output as a csv and imported in R. From here, we began our NLP processes,  which will be detailed below. 

Our analysis included metrics of word frequency (by part of speech and decade), sentiment analysis, lexical diversity and density, bigrams, term frequency - inverse document frequency (TF-IDF) importance analysis, and latent dirichlet allocation (LDA). Our visualization tools included wordclouds, heatmaps, radar charts, bar-graphs, and smooth line plots over time.


# Results and Findings
[Please be aware that crude language is included in this analysis]


## Word Clouds

Word Clouds are designed to visualize the words appearing with the highest frequency. We believe that this visualization best displays the most commonly used words in our song lyrics dataset on a total and per-decade basis and are useful in determining broad changes in lyrical language over time. In order to create this visualization, we first create the ‘Corpus’ column of our data, which removes meaningless words like articles and prepositions, in order to better highlight the more impactful words (nouns, verbs, and adjectives) of songs. We then tokenize (break up by word) our existing song data to only consider each “Corpus” word and the number of times that word appears in our fifty-year time period. We create an initial word cloud reflecting the top ten popular words of all-time. 


```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(stringr)
library(wordcloud2)
library(tidytext)
library(Rcpp)
```

```{r, message=FALSE}
songtoken <- songs %>%
  select(Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy <- songtoken %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))
```

```{r, echo=FALSE}
wordcloud2(data = song_tidy[1:100,], size = 1, color = "random-light")
```


Then, we separate the data by decade to determine if certain words have popularity peaks in different eras. Consistent across all decades, love is the most commonly used word in our lyrical database. At the turn of the century, we notice there is a rise in profanity and offensive terminology. Otherwise, the word clouds organized by decade show no other major changes in word frequency.

```{r, echo=FALSE, message=FALSE}
songs70 <- songs %>%
  filter(Decade == 1970)

songs80 <- songs %>%
  filter(Decade == 1980)

songs90 <- songs %>%
  filter(Decade == 1990)

songs00 <- songs %>%
  filter(Decade == 2000)

songs10 <- songs %>%
  filter(Decade == 2010)

yearlist <- c(1970:2019)

top10 <- c()
for(i in c(1:10)){
  top10 <- c(top10, song_tidy[i, 1])
}

masterwordlist <- data.frame()


for (i in yearlist){
  
yearsongtoken <- songs %>%
  filter(Year == i) %>%
  select(Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

year_song_tidy <- yearsongtoken %>% 
  filter(word == top10[1] | word == top10[2] | word == top10[3] |word == top10[4] |word == top10[5] |word == top10[6] |word == top10[7] |word == top10[8] |word == top10[9] |word == top10[10]) %>%
  group_by(word) %>%
  count() %>% 
  arrange(desc(n)) %>%
  mutate(Year = i)

if (length(masterwordlist) == 0 ){
  masterwordlist <- year_song_tidy 
}else{
  
masterwordlist <- merge(year_song_tidy, masterwordlist, all = TRUE)
  }
}
```

```{r, echo=FALSE, message=FALSE}

songtoken70 <- songs70 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy70 <- songtoken70 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

wordcloud2(data = song_tidy70[1:100,], size = 1, color = "random-light")


songtoken80 <- songs80 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy80 <- songtoken80 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

wordcloud2(data = song_tidy80[1:100,], size = 1, color = "random-light")


songtoken90 <- songs90 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy90 <- songtoken90 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

wordcloud2(data = song_tidy90[1:100,], size = 1, color = "random-light")



songtoken00 <- songs00 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy00 <- songtoken00 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

wordcloud2(data = song_tidy00[1:100,], size = 1, color = "random-light")



songtoken10 <- songs10 %>%
  select(Song.Title, Nouns) %>%
  unnest_tokens(output = word, input = Nouns)

song_tidy10 <- songtoken10 %>% 
  group_by(word) %>%
  count() %>% 
  arrange(desc(n))

wordcloud2(data = song_tidy10[1:100,], size = 1, color = "random-light")
```

## Top 10 Words and Heat Map

To see whether artists’ popular language change across the decades, we plot the top ten most commonly used words from 1970 to 2019. To build the plot, we examine our complete tokenized word list to determine the ten most-used words over the past fifty years. We then count the instances in which those words were used in each distinct year and plot the results as a line graph below. 


```{r, echo=FALSE, message=FALSE}
ggplot(masterwordlist, aes(x = Year, y = n, color = word)) + geom_line()

```


This graph provides trends of popular words over time, and we observe that “love” dominates our dataset, although it declines in frequency as we approach the present era. In the late 1980s and early 2000s, we see “baby” and “girl,” respectively, begin to gain more traction, indicating a shift in music to a slightly more diverse set of lyrics. Providing an even better visual of these changes is the heat map below, where the lighter shade indicates higher popularity.


```{r, echo=FALSE, message=FALSE}
library(gplots)
library(dplyr)
library(tidyverse)

arrangedmasterwordlist <- masterwordlist %>%
  group_by(word) %>%
  arrange((Year))
```

```{r, echo=TRUE, message=FALSE}
widemasterwordlist <- data.frame(pivot_wider(arrangedmasterwordlist, id_cols = word, names_from = Year, values_from = n))
row.names(widemasterwordlist) <- widemasterwordlist$word
masterwordmatrix <- data.matrix(widemasterwordlist[,-1])

wordheatmap <- heatmap(masterwordmatrix, Rowv = NA, Colv = NA, col = heat.colors(256), scale = "column", margins = c(5, 10))
```

```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(tidyr) 
library(widyr)
library(ggplot2)
library(ggrepel) 
library(gridExtra) 
library(knitr) 
library(kableExtra) 
library(formattable)
library(circlize) 
library(tidytext) 
library(textdata)

afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
nrc <- get_sentiments("nrc")
```


## Sentiment Radar Graph By Decade

To gain a deeper understanding of how general musical moods have changed over time, we run a sentiment analysis on our song lyrics over the past five decades. In carrying out this analysis, we note that oftentimes a singular word lacks requisite context to provide a proper analysis of its usage. So, we use a method that aggregates the sentiments of all words in a decade to better discover overall themes. 

We use the R package ‘tidytext’ to implement a dictionary using the NRC (National Research Council in Canada) sentiment analysis dataset, which places given words into ten buckets, including joy, fear, disgust, anticipation, anger, trust, surprise, sadness, positive, and negative. Note that the figures below feature only eight buckets. Since we evaluate positive and negative sentiment later, we removed those buckets from our NRC sentiment analysis. We joined our summarized data-set by word with this dictionary in order to determine the percentage values for each sentiment within each corpus for each decade, using the number of words in each bucket as a percentage of the total number of words. We then plot the percentage values on five (one per-decade) distinct radar charts. Below is an example of code we use to calculate the 1970 radar chart.

```{r, eval=FALSE, echo=TRUE, message=FALSE}
song70_nrc <- song_tidy70 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_70s <- song70_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "1970s Sentiment")
```

As seen on the charts below, we notice the sentiment distributions for the 70’s, 80’s and 90’s are rather similar, underscored with high levels of joy and moderate levels of anticipation and trust. Lyrical sentiment begins to change at the turn of the century, as the sentiment levels for anger, fear, disgust and sadness all begin to spike. This is once again indicative of the phenomenon observed in previous visualizations documenting the decline of the word “love” and the rise of more profane words. Overall, songs appear to increase in sentiment complexity over time (the red area grows).


```{r, echo=FALSE, message=FALSE}
library(textdata)
library(tidytext)
library(radarchart)

nrc <- get_sentiments("nrc")
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")


song70_nrc <- song_tidy70 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_70s <- song70_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "1970s Sentiment")

radar_chart_70s

song80_nrc <- song_tidy80 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_80s <- song80_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "1980s Sentiment")

radar_chart_80s

song90_nrc <- song_tidy90 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_90s <- song90_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "1990s Sentiment")

radar_chart_90s

song00_nrc <- song_tidy00 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_00s <- song00_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "2000s Sentiment")

radar_chart_00s

song10_nrc <- song_tidy10 %>% 
  inner_join(nrc) %>% 
  filter(!sentiment %in% c("negative", "positive")) %>% 
  group_by(sentiment) %>%
  summarise(Sent_Count = sum(n)) %>% 
  mutate(Sentiment = (Sent_Count/sum(Sent_Count))*100)

radar_chart_10s <- song10_nrc %>%
  select(-Sent_Count) %>% 
  chartJSRadar(showToolTipLabel = TRUE, main = "2010s Sentiment")

radar_chart_10s
```


## Positive and Negative Word Clouds

To further explore and verify what was leading to the rise in anger, fear, disgust and sadness in the twenty-first century, we continue our sentiment analysis to observe how positively and negatively-rated words change over the decades. Through word cloud visualization, we can see which negative words contribute most to the rise in negative sentiment in the 2000s and 2010s. 

We use the ‘tidytext’ package as before, but this time decide to use the “bing” sentiment analysis (puts words into two buckets: positive or negative) method instead of NRC (puts words into ten buckets as listed above) and additionally use the ‘wordcloud2’ package to plot the resulting word clouds. As shown below, the word clouds are separated in half, with positive words on the lower half and negative words on the upper half. The first plot is for the 1970s, the second for the 2010s. In the Appendix, we plot each decade.


```{r, echo=FALSE, message=FALSE}
library(reshape2)
library(wordcloud)


song_compcloud70 <- songtoken70 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

```{r, echo=FALSE, eval=FALSE, message=FALSE}
song_tidy80 <- songtoken80 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r, echo=FALSE, eval=FALSE, message=FALSE}
song_tidy90 <- songtoken90 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r, echo=FALSE, eval=FALSE, message=FALSE}
song_tidy00 <- songtoken00 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r, echo=FALSE, message=FALSE}
song_tidy10 <- songtoken10 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```


“Love” is prominently displayed in the middle of all word clouds, once again emphasizing its importance. As expected, we see a rise of profanity, aggression, and other offensive terms (which classify as negative sentiment) in the 2000s and 2010s, contributing to the previously noticed sentiments of anger, sadness, and disgust. Possible reasons for such agitation could include the dawn and duration of a war from 2001 to 2011, the bursting of the tech bubble in the early 2000s, the mortgage meltdown financial crisis of 2008, and political polarization in the 2010s.


## Lexical Diversity By Year

Lexical diversity describes the use of unique words over time. “Unique” words are defined as the number of distinct words, across all songs, in a given time frame (each decade, for our case). The visualization below communicates how the complexity of song lyrics have changed over time, i.e., with more unique words comes more song lyric complexity. From our sentiment analysis radar charts above, we recall that the moods of songs has grown more complex (the red area of the radar charts has grown). Therefore, we expect to see an overall increase in lexical diversity. It would make sense that the more complex lyrics are, the more moods and sentiments they communicate. Again, we use our tokenized word list and simply count the number of words used each year by counting the number of rows per year. We then plot these results over time. 


```{r, echo=FALSE, message=FALSE}
my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00")

theme_lyrics <- function() 
{
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_blank(), 
        axis.ticks = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none")
}
```

```{r, echo=TRUE,message=FALSE}
lex_diversity_per_year <- songs %>%
  unnest_tokens(word, Lyrics) %>%
  group_by(Song.Title,Year) %>%
  summarise(lex_diversity = n_distinct(word)) %>%
  arrange(desc(lex_diversity)) 
```

```{r, echo=FALSE, message=FALSE}
diversity_plot <- lex_diversity_per_year %>%
  ggplot(aes(Year, lex_diversity)) +
    geom_point(color = my_colors[3],
               alpha = .3, 
               size = 1, position = "jitter") + 
    stat_smooth(color = "black", se = FALSE, method = "lm") +
    geom_smooth(aes(x = Year, y = lex_diversity), se = FALSE,
                color = "blue", lwd = 2) +
    ggtitle("Lexical Diversity") +
    xlab("Year") + 
    ylab("") +
    scale_color_manual(values = my_colors) +
    theme_classic() + 
    theme_lyrics()

diversity_plot
```


We see a slight upward trend in lexicon diversity, meaning song lyrics have grown more complex over the past fifty years, in terms of the number of unique words in a given decade. This is consistent with our sentiment analysis findings.


## Lexical Density By Year

Lexical density, the ratio of unique words to total words in a given time period, pairs well with lexical diversity. Lexical density analysis allows us to observe lyrical complexity as before, but now we look at lyrical complexity in comparison to the total number of words used (across all songs) per year. Again, we find the number of unique words per year by counting the number of rows in each respective year, then divide by the total count (n) of the sum of all of those words. 


```{r, echo=TRUE, message=FALSE}
lex_density_per_year <- songs %>%
  unnest_tokens(word, Lyrics) %>%
  group_by(Song.Title,Year) %>%
  summarise(lex_density = n_distinct(word)/n()) %>%
  arrange(desc(lex_density))
```

```{r,echo=FALSE, message=FALSE}
density_plot <- lex_density_per_year %>%
  ggplot(aes(Year, lex_density)) + 
    geom_point(color = my_colors[4],
               alpha = .4, 
               size = 2, 
               position = "jitter") + 
    stat_smooth(color = "black", 
                se = FALSE, 
                method = "lm") +
    geom_smooth(aes(x = Year, y = lex_density), 
                  se = FALSE,
                  color = "blue", 
                  lwd = 2) +
      ggtitle("Lexical Density") + 
      xlab("Year") + 
      ylab("") +
      scale_color_manual(values = my_colors) +
      theme_classic() + 
      theme_lyrics()

density_plot
```

The results yield a downward trend, indicating that the number of total words used in songs has increased even more drastically than the number of unique words has increased over time, implying that songs have become more verbose. Since the increase in the number of total song words outweighs the increase in the number of unique words, we believe songs have also become more repetitive over time.


## Word Importance By Decade and Bigram Map

One major goal of this project is to see what lyrics make a song popular. In addition to the analyses above, we can also determine what words are most “important” to various songs in the Top 100 by answering the question: given a song in the Top 100, which words occur most in that song? To answer this question, we use a Term Frequency - Inverse Document Frequency (TF-IDF) methodology to rank words in terms of “importance.” 

TF-IDF looks at the frequency of a certain term in the dataset among all songs, and the number of songs that word appears in. A word of high “importance” would have a high frequency but only occur in a limited number of songs, indicating that the word was instrumental (pun intended) to the popularity of that song. A word of low “importance” could also have a high frequency but occur in several different songs, indicating that the word is sort of a “jack of all trades, master of none.” For example, words like “girl” would have low importance, because although “girl” has high frequency, it appears in several songs, so its frequency-in-song is low. To implement this system, we use the R package ‘tidytext’ and the function ‘bind_tfidf()’, which automatically calculates word importance values. 

```{r, echo=FALSE, message=FALSE}
library(tidytext) 
library(RColorBrewer)

tfidf_words_decade <- songs %>%
  unnest_tokens(word, Lyrics) %>%
  distinct() %>%
  filter(nchar(word) > 3) %>%
  count(Decade, word, sort = TRUE) %>%
  bind_tf_idf(word, Decade, n) %>%
  arrange(desc(tf_idf))

top_tfidf_words_decade <- tfidf_words_decade %>% 
  group_by(Decade) %>% 
  slice(seq_len(8)) %>%
  ungroup() %>%
  arrange(Decade, tf_idf) %>%
  mutate(row = row_number())

top_tfidf_words_decade %>%
  ggplot(aes(x = row, tf_idf, fill = Decade)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Decade") +
    theme_lyrics() +  
    facet_wrap(~Decade, 
               ncol = 3, nrow = 2, 
               scales = "free") +
    scale_x_continuous(  # this handles replacement of row 
        breaks = top_tfidf_words_decade$row, # notice need to reuse dataframe
        labels = top_tfidf_words_decade$word) +
    coord_flip()
```


As alluded to above, looking at singular word importance does not provide relevant information for our analysis due to the complexity of language and ambiguous contextualization of words. So, we decided to create bigrams (pairs of words) to help contextualize some of the lyrical language.  Once again, we use the R package ‘tidytext’, and we tokenize the lyrics pairwise instead of singularly. In the following visualizations, we see that “love” is prominent in the first three decades considered, paired with multiple other words such as “baby” and “fall.” 

```{r, echo=TRUE, message=FALSE}
song_bigrams <- songs %>%
  unnest_tokens(bigram, Corpus, token = "ngrams", n = 2)

bigrams_separated <- song_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")
```

```{r, echo=FALSE, message=FALSE}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  filter(nchar(word1) > 2) %>%
  filter(nchar(word2) > 2)
 

bigram_decade <- bigrams_filtered %>%
  filter(word1 != word2) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, Decade, sort = TRUE) %>%
  group_by(Decade) %>%
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(Decade, n) %>%
  mutate(row = row_number())

```

```{r,echo=FALSE, message=FALSE}
ggplot(bigram_decade, aes(x= row, y = n, fill = Decade)) + geom_col(show.legend = FALSE) + facet_wrap(~Decade, scales = "free_y") +
  xlab(NULL) + ylab(NULL) + scale_x_continuous(breaks = bigram_decade$row, labels = bigram_decade$bigram) + theme(panel.grid.major.x = element_blank()) + ggtitle("Bigrams Per Decade") + coord_flip()
```

Again, as we move to the turn of the century, the word importance chart becomes dominated by combinations of more profane words. To help visualize word importance, we observe a map of nodes showing how various words link to one another. Using the R package ‘ggraph’, we were able to plot bigrams of high importance. In the visualization, nodes that have many links are used in a variety of ways with multiple other words, proving their universal importance, whereas links on the outer edges of the map containing only two connections indicate bigrams that only prove to be important in a few individual cases. “Love” and “baby” have multiple important connections, but words like “free” only have one meaningful partner. 


```{r, echo=FALSE, message=FALSE}
library(ggraph)

bigram_decade %>% select(bigram, n) %>%
  separate(bigram, c("word1", "word2")) %>%
  ggraph(layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


Even when using bigrams, the nuances of language are not always well-displayed. For example, one repetitive song of the 2000’s, “Crank That” by Soulja Boy, features a repetitive and self-referential chorus and verse and was featured twice in the Top 100. Therefore, because many of the phrases used in this song were unique to this song and repeated many times, it was given an extraordinarily high TF-IDF bigram importance value, earning it a spot on the top words of the decade. We attempted to remedy this issue by removing some of these problematic words but ultimately found that even after doing so, the bigrams would be replaced by other, equally unrefined words such as names of artists like “Justin” and “Timberlake,” so we remain limited as to how much information we can draw from this technique. 


In an attempt to characterize the lyrical choices of what we deemed “top artists,” we isolated the 25 artists who have the most multi-year (and even multi-decade) occurrences of music across Billboards. We then aimed to examine their lexicon and see if there are any distinguishing features that could point to their success as musicians. However, despite attempts to model their top words, most important words, and most important bigrams compared to the global array of artists, we could not find any discernible differences between the corpuses used and could draw no serious conclusions for the visualizations we created (see appendix).


## Latent Dirichlet Allocation and Time-Course Plots

Latent Dirichlet Allocation (LDA) is a probabilistic model used in Natural Language Processing that relies on unsupervised learning to group words into a pre-set number of clusters based on “relatedness.” The goal of this analysis is to group terms that commonly co-occur in songs into topics, which can be thoughtfully assessed more holistically with a human eye in order to draw meaning from the remote associations of words. We did the following for nouns, verbs, and adverbs.

```{r, echo=TRUE, message=FALSE}
song_noun <- songs %>% 
  dplyr::select(Song.Title, Nouns, Year) %>% 
  unnest_tokens(output = word, input = Nouns) %>% 
  mutate(Upos = "NOUN") %>% 
  mutate(Year = Year)
```

We then created a "part of speech" placeholder that binded the three.

The algorithm first assigns words to a random topic and then it iteratively reassigns each word to a new topic and calculates the probability that a word belongs to a given topic and also the probability that the document (in our case a song) could be classified to a particular topic (Lettier 2018). In order to formulate our document-term matrix, the necessary input to an LDA model, which includes rows that are our documents (songs) and columns that are each word in a corpus, we turn to Python’s ‘gensim’ and ‘nltk’ packages, in order to tokenize and “stem” each word within each song. Stemming removes prefixes and suffixes from words that have the same base in order for them to be matched within and between songs. 

From here, we are able to run the LDA model for a variety of different numbers of topics and numbers of words per topic. We eventually decide on a final number of topics: eight, with five terms per topic. Although there are ways to “optimize” the number of topics, we thought that manually evaluating the success by making sense of the words included in each topic was a better way of approaching our algorithmic selection. (We also attempted to use LDA modeling by decade but saw less success.) The below is code within our topic modeling function, which is a function that arranges topics (like passion) by words that are associated with that topic.

```{r, echo=TRUE, eval=FALSE, message=FALSE}
topic_plot <-  song_upos %>%
  filter(word %in% topic) %>%
  group_by(Year) %>%
  mutate(topic_count = n()) %>%
  select(Year, topic_count) %>%
  distinct() %>%
```

```{r,echo=FALSE, message=FALSE}
song_verb <- songs %>% 
  dplyr::select(Song.Title, Verbs, Year) %>% 
  unnest_tokens(output = word, input = Verbs)%>% 
  mutate(Upos = "VERB") %>% 
  mutate(Year = Year)

song_adverb <- songs %>% 
  dplyr::select(Song.Title, Adverbs, Year) %>% 
  unnest_tokens(output = word, input = Adverbs)%>% 
  mutate(Upos = "ADV") %>% 
  mutate(Year = Year)

song_upos <- rbind(rbind(song_noun,song_verb), song_adverb)
```


```{r,echo=FALSE, message=FALSE}

t1 <- c("woah", "kiss", "life", "thing", "world", "song", "people") 

t2 <- c("tonight", "dance", "party", "hand", "thing")

t3 <- c("friend", "head", "woman", "lover", "lady", "ladies")

t4 <- c("shit", "bitch", "bottom", "water", "pussi", "pussy")

t5 <- c("money", "bodies", "type", "bitch", "cash", "body")

t6 <- c("name", "chance", "town", "rain", "tear")

t7 <- c("gang", "taste", "girlfriend", "wrist", "chain") 

t8 <- c("thunder", "star", "murder", "ghost", "wish")
```

```{r, echo=FALSE, message=FALSE}
topic_modeling <- function(topic, color, title){

  topic_plot <-  song_upos %>%
  filter(word %in% topic) %>%
  group_by(Year) %>%
  mutate(topic_count = n()) %>%
  select(Year, topic_count) %>%
  distinct() %>%
  ggplot(aes(Year, topic_count)) + geom_smooth(se = FALSE, col = color) + ggtitle(title)
  topic_plot
}

colors <- c("#89C5DA", "#DA5724", "#74D944", "#CE50CA", "#3F4921", "#C0717C", "#CBD588", "#5F7FC7", 
"#673770", "#D3D93E", "#38333E", "#508578", "#D7C1B1", "#689030", "#AD6F3B", "#CD9BCD", 
"#D14285", "#6DDE88", "#652926", "#7FDCC0", "#C84248", "#8569D5", "#5E738F", "#D1A33D", 
"#8A7C64", "#599861")
```

Ultimately the eight groupings we decided on, which are detailed below, are classified based on our associations with the words included in each topic: “Passion/Longing, Party/Dance/Movement, Intimacy/Women, Sex/Objectification, Measurement of Clout, Impulsivity/Sadness, Committment/Tether, Faith/Violence.” 

For each topic, we then graph the occurrence of these themes over the course of our fifty year window. In the “Passion/Longing” grouping, we see an increase in occurrence over time, with a peak in the early 2000’s, before a steep decline in the late 2000’s and 2010’s. 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t1, sample(colors, 1), "Passion/Longing")
```

For “Party/Dance/Movement,” there is an increase, followed by a plateau in the mid 1980’s, and then another large increase and peak in the late 2000s, before a final decrease in the 2010’s. 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t2, sample(colors, 1), "Party/Dance/Movement")
```

The “Intimacy/Women” topic begins high, declines in the 1980’s, and then ticks back up and peaks near the year 2000 before starting a steep descent through the 2000’s and 2010’s. 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t3, sample(colors, 1), "Intimacy/Women")
```

“Sex/Objectification” and “Measurements of Clout” follow a similar pattern, with a gradual increase over time with their steepest increase coming in the 2010’s. 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t4, sample(colors, 1), "Sex/Objectification")
```
```{r,echo=FALSE, message=FALSE}
topic_modeling(t5, sample(colors, 1), "Measurements of Clout")
```

“Impulsivity/Sadness” is bimodal in nature, with peaks in the early 1980’s and 2000’s and steep declines on either side, with a smaller decline between the peaks. 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t6, sample(colors, 1), "Impulsivity/Sadness")
```

“Commitment/Tether” shows little increase over time until the late 2000’s, where it shows a sharp increase. Lastly, 

```{r,echo=FALSE, message=FALSE}
topic_modeling(t7, sample(colors, 1), "Commitment/Tether")
```

“Faith/Violence”  begins a gradual increase over the 1970s-1980s, with a plateau from 1990 to the late 2000’s, and a steep increase in the 2010’s.

```{r,echo=FALSE, message=FALSE}
topic_modeling(t8, sample(colors, 1), "Faith/Violence")
```

In interpreting these results, we examine general trends in the types of songs that were in the Billboard Top 100 over time. Despite the upward trend in lexical diversity over time, we see that songs included in the Billboard Top 100 may have increasing messages of superficiality, with fewer trends dealing with emotive, love-oriented concepts. We see, instead, a shift away from songs that deal with loved-ones to a stronger focus on social-status, objectification, violence, and a relationship viewpoint that prioritizes comrades over romantic partners, noting the tethering nature of intimacy. 

These results parallel those of our sentiment analysis, which find high levels of joy across decades and feature greater amounts of anger, fear and disgust in the 2010’s relative to other decades. It is apparent that utilizing LDA does offer a more nuanced way of analyzing sentiment beyond using pre-existing dictionaries of words. These observed trends, which require human intervention for their formulation, are not without bias. Additionally, the trends above may serve as poor forecasts for the overall trend in music and instead may be limited to the songs featured in the Billboard Top 100.



# Work Limitations

Our findings are indubitably interesting, yet we are also excited about a few potential next steps. First, connotations of words in the English language have certainly changed over the past fifty years. For example, “sick” only started meaning “cool” around the mid-1980s as a result of West Coast skate/surf culture. Thus, the word “sick” could have both a positive (if it means “cool”) and a negative (if it means “ill”) sentiment in songs after the mid-1980s. Creating sentiment analysis data by decade and comparing to song lyrics by decade would alleviate a good portion of this issue. Our current sentiment analysis charts above show a fairly significant increase in anger, disgust, and sadness from the early 2000s to the present, mainly due to an increase in profane, negative, and aggressive words. While this increase in negative sentiment could be reflective of the general economic and political agitation of the 2000s and 2010s, it could also be the case that the use of profanity and such aggressive language is more common and socially acceptable now than it was in the late twentieth century. Therefore, breaking down sentiment by decade would provide interesting and more accurate analyses of the respective decade sentiments.

Second, we would like to use song lyrics to classify songs by genre. For example, we found an extremely interesting interactive site (https://pudding.cool/2017/09/hip-hop-words/) that outputs the “most Hip-Hop” (like “stunting”) and “least Hip-Hop” (like “desire”) words, among other things. If we could acquire data on the genre of each of the songs in our dataset, we could create these “most [insert genre]” and “least [insert genre]” lists over the last fifty years, which would be fascinating. Neither the websites we scraped nor the Genius API contained data for song genre. We tried to utilize the Itunes API, which did have genre data, but the Itunes API had embedded restrictions that would have made the process extremely inefficient and computationally challenging. Given more time, we would be interested in scraping the genre data and breaking down the above outputs over the last five decades to analyze genre and genre word popularity over time.

Lastly, it would be interesting to gather a larger dataset of songs in and out of the Billboard Top 100 to analyze the differences between charted (Top 100) and uncharted (not Top 100) songs. Opposite our analysis above, perhaps there are certain words used in songs that significantly prevent a song from being in the Top 100. We could also use rankings of the charted songs to interpret word differences between songs in the top and bottom quartiles.


# Ethical Considerations

We encountered certain ethical considerations almost immediately in our research and results. As seen above, some of the most popular song lyric words over the years are profane and can certainly be deemed offensive. For example, words like the n-word and a derogatory term used to describe women have become increasingly popular over time in song lyrics, and would appear in our word cloud. As a group, we found it difficult to draw an exact line for which of these kinds of words to keep and which to exclude. Ultimately, we decided that since profanity and similarly offensive words are useful in identifying sentiment and therefore helpful to our study, we would include such words in our results, but not without a warning of such occurrences.

In addition, our group recognizes that Billboard calculates its “Top 100” songs based on the sales, radio play, and online streaming rankings of all songs (Billboard, 2018). As a result, these Top 100 popularity charts are indicative mostly of the song preferences of a portion of the population that is able to spend the time and money to purchase and listen to these songs. As a result, while we can certainly communicate interesting findings of song lyric popularity over time, we must be careful when generalizing to the population, since there exist less privileged groups whose preferences may not be reflected in the Top 100.


# Conclusion

Overall, popular song lyrics over the past fifty years are able to tell so much beyond just what makes a song popular. We were able to make conclusions and advanced analyses about popular words, significant sentiments, song complexity, and related concepts and topics over time. Our group really enjoyed working together on this project and remains ambitious to explore our next steps!





# Appendix


## Data Acquisition and Cleaning

```{r, echo=FALSE, message=FALSE}
library(reticulate)
```

```{python, eval = FALSE, echo=FALSE, message=FALSE}
import pandas as pd
import numpy as np
import requests
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import HTML, display
from bs4 import BeautifulSoup
from nltk.tokenize import RegexpTokenizer
import lyricsgenius as genius
import sys
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from nltk.stem import PorterStemmer
from datetime import datetime
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
from collections import Counter
from os import path
from PIL import Image
from keras.models import model_from_json
import pickle
import json
```

```{r, echo=TRUE, eval=FALSE, message=FALSE}
def collect_songs_from_billboard(start_year,end_year):
    '''This function takes in a start year and and end year, then iterates through each year to 
    pull song data from billboard or bobborst as needed. Then it uses beautiful soup to clean
    the data. Finally it stores the cleaned data in a dataframe and returns it
    
    Parameters:
    
    start_year (int): the year to start at.
    end_year (int): the year to end at.
    Returns: 
    
    dataframe.
    '''
    years = np.arange(start_year, end_year + 1).astype(int)
    dataset = pd.DataFrame()
    url_list = []
    all_years = pd.DataFrame()
    final_years = np.arange(2013,2020)
    ### Billboard doesn't have it's own complete results from 1970 to 2016,
    ### so we'll use bobborst.com as our primary and collect from billboard as needed
    #URL Constructor
    for i in range (0, len(years)):
        url_list.append("http://billboardtop100of.com/" + str(years[i]) + "-2/")      
    for i in range(0, len(url_list)):
        if years[i] in final_years:
            sys.stdout.write("\r" + "Collecting Songs from " +str(years[i]) + " via https://www.billboard.com")
            sys.stdout.flush()
            url = "https://www.billboard.com/charts/year-end/" + str(years[i]) + "/hot-100-songs"
            page = requests.get(url)
            soup = BeautifulSoup(page.content, "html.parser")
            all_ranks = soup.find_all("div", class_="ye-chart-item__rank")
            all_titles = soup.find_all('div', class_="ye-chart-item__title")
            all_artists = soup.find_all("div", class_="ye-chart-item__artist")
            for j in range (0, len(all_ranks)):
                row = {
                    "Rank": all_ranks[j].get_text(strip=True),
                    "Song Title": all_titles[j].get_text(strip=True),
                    "Artist": all_artists[j].get_text(strip=True),
                    "Year": years[i]
                }
                dataset = dataset.append(row, ignore_index=True)
        else:
            sys.stdout.write("\r" + "Collecting Songs from " +str(years[i]) + " via https://www.billboard.com")
            sys.stdout.flush()
            url = "http://billboardtop100of.com/" + str(years[i]) + "-2/"
            page = requests.get(url)
            soup = BeautifulSoup(page.content, "html.parser")
            table = soup.find_all('tr')
            for j in range(0, len(table)):
                columns = table[j].find_all('td')
                row = {
                    "Rank": columns[0].get_text(strip=True),
                    "Artist": columns[1].get_text(strip=True),
                    "Song Title": columns[2].get_text(strip=True),
                    "Year": years[i]
                }
                dataset = dataset.append(row, ignore_index=True)
    dataset['Year'] = dataset['Year'].astype(int)
    return dataset


def add_spacy_data(dataset, feature_column):
    '''
    Grabs the verb, adverb, noun, and stop word Parts of Speech (POS) 
    tokens and pushes them into a new dataset. returns an 
    enriched dataset.
    
    Parameters:
    
    dataset (dataframe): the dataframe to parse
    feature_column (string): the column to parse in the dataset.
    
    Returns: 
    dataframe
    '''
    
    verbs = []
    nouns = []
    adverbs = []
    corpus = []
    nlp = spacy.load('en_core_web_sm')
    ##
    for i in range (0, len(dataset)):
        print("Extracting verbs and topics from record {} of {}".format(i+1, len(dataset)), end = "\r")
        song = dataset.iloc[i][feature_column]
        doc = nlp(song)
        spacy_dataframe = pd.DataFrame()
        for token in doc:
            if token.lemma_ == "-PRON-":
                    lemma = token.text
            else:
                lemma = token.lemma_
            row = {
                "Word": token.text,
                "Lemma": lemma,
                "PoS": token.pos_,
                "Stop Word": token.is_stop
            }
            spacy_dataframe = spacy_dataframe.append(row, ignore_index = True)
        verbs.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "VERB"].values))
        nouns.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "NOUN"].values))
        adverbs.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "ADV"].values))
        corpus_clean = " ".join(spacy_dataframe["Lemma"][spacy_dataframe["Stop Word"] == False].values)
        corpus_clean = re.sub(r'[^A-Za-z0-9]+', ' ', corpus_clean)   
        corpus.append(corpus_clean)
    dataset['Verbs'] = verbs
    dataset['Nouns'] = nouns
    dataset['Adverbs'] = adverbs
    dataset['Corpus'] = corpus
    return dataset

lyric_output = []

def pre_clean(dataset):
    for i in range(0,len(dataset)):
        oldlyric = dataset.iloc[i]['lyrics']
        newlyric = re.sub(r'[^A-Za-z0-9]+', ' ', oldlyric)
        lyric_output.append(newlyric)
    dataset['lyrics'] = lyric_output
    return dataset

def prep_corpus(raw_string):
    '''Single use of add_spacy_data to enable pipelining 
    data into predictions
    
    Parameters:
    raw_string (string): String to be parsed
    
    Returns:
    parsed string
    '''

    verbs = []
    nouns = []
    adverbs = []
    corpus = []
    nlp = spacy.load('en_core_web_sm')

    doc = nlp(raw_string)
    spacy_dataframe = pd.DataFrame()
    for token in doc:
        if token.lemma_ == "-PRON-":
                lemma = token.text
        else:
            lemma = token.lemma_
        row = {
            "Word": token.text,
            "Lemma": lemma,
            "PoS": token.pos_,
            "Stop Word": token.is_stop
        }
        spacy_dataframe = spacy_dataframe.append(row, ignore_index = True)
    verbs.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "VERB"].values))
    nouns.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "NOUN"].values))
    adverbs.append(" ".join(spacy_dataframe["Lemma"][spacy_dataframe["PoS"] == "ADV"].values))
    corpus_clean = " ".join(spacy_dataframe["Lemma"][spacy_dataframe["Stop Word"] == False].values)
    corpus_clean = re.sub(r'[^A-Za-z0-9]+', ' ', corpus_clean)   

    return corpus_clean

all_songs = collect_songs_from_billboard(1970, 2019)

all_songs["Artist"][all_songs['Artist'] == "Jackson 5"] = "The Jackson 5"
all_songs["Artist"][all_songs['Artist'] == "Beatles"] = "The Beatles"

api = genius.Genius("Gk7JH9g31J9T2nWV-o82WaGwIZQ_04LgbxcJypt4dBRdaGSH494rBORd2qMIVlzJ",sleep_time=0.01, verbose=False)

all_song_data = pd.DataFrame()
start_time = datetime.now()
print("Started at {}".format(start_time))
for i in range(0, len(all_songs)):
    rolling_pct = int((i/len(all_songs))*100)
    print(str(rolling_pct) + "% complete." + " Collecting Record " + str(i) +" of " +
          str(len(all_songs)) +". Year " + str(all_songs.iloc[i]['Year']) + "." + " Currently collecting " + 
          all_songs.iloc[i]['Song Title'] + " by " + all_songs.iloc[i]['Artist'] + " "*50, end="\r")
    song_title = all_songs.iloc[i]['Song Title']
    song_title = re.sub(" and ", " & ", song_title)
    song_title_test = re.sub(r'\W+', '', song_title).lower()
    artist_name = all_songs.iloc[i]['Artist']
    artist_name = re.sub(" and ", " & ", artist_name)

    try:
        song = api.search_song(song_title, artist=artist_name)
        result_title = re.sub(r'\W+', '', song.title).lower()
        if result_title == song_title_test:
            song_album = song.album
            song_album_url = song.album_url
            featured_artists = song.featured_artists
            song_lyrics = re.sub("\n", " ", song.lyrics) #Remove newline breaks, we won't need them.
            song_media = song.media
            song_url = song.url
            song_writer_artists = song.writer_artists
            song_year = song.year
        else:
            print(song_title)
            print(result_title)
            song_album = "null"
            song_album_url = "null"
            featured_artists = "null"
            song_lyrics = "null"
            song_media = "null"
            song_url = "null"
            song_writer_artists = "null"
            song_year = "null"
    except:
        song_album = "null"
        song_album_url = "null"
        featured_artists = "null"
        song_lyrics = "null"
        song_media = "null"
        song_url = "null"
        song_writer_artists = "null"
        song_year = "null"
        
    row = {
        "Year": all_songs.iloc[i]['Year'],
        "Rank": all_songs.iloc[i]['Rank'],
        "Song Title": all_songs.iloc[i]['Song Title'],
        "Artist": all_songs.iloc[i]['Artist'],
        "Album": song_album,
        "Album URL": song_album_url,
        "Featured Artists": featured_artists,
        "Lyrics": song_lyrics,
        "Media": song_media,
        "Song URL": song_url,
        "Writers": song_writer_artists,
        "Release Date": song_year
    }
    all_song_data = all_song_data.append(row, ignore_index=True)
end_time = datetime.now()
print("\nCompleted at {}".format(start_time))
print("Total time to collect: {}".format(end_time - start_time))


all_song_data.to_csv("all_songs_data.csv")
all_song_data.to_json("all_song_data.json", orient='records')
loaded_song_dataset = pd.read_csv("all_songs_data.csv",index_col=0)

songs_with_lyrics_dataset = loaded_song_dataset.dropna(subset=['Lyrics'])
prepared_songs_dataset = add_spacy_data(songs_with_lyrics_dataset, 'Lyrics')

prepared_songs_dataset = prepared_songs_dataset.drop(columns = ['Unnamed: 0'])
word_counts = []
unique_word_counts = []
for i in range (0, len(prepared_songs_dataset)):
    word_counts.append(len(prepared_songs_dataset.iloc[i]['Lyrics'].split()))
    unique_word_counts.append(len(set(prepared_songs_dataset.iloc[i]['Lyrics'].split())))
prepared_songs_dataset['Word Counts'] = word_counts
prepared_songs_dataset['Unique Word Counts'] = unique_word_counts

prepared_songs_dataset = pd.read_csv('prepped_data.csv', index_col=0)

summary_dataset = pd.DataFrame()
years = prepared_songs_dataset['Year'].unique().tolist()
for i in range(0, len(years)):
    row = {
        "Year": years[i],
        "Average Words": prepared_songs_dataset['Word Counts'][prepared_songs_dataset['Year'] == years[i]].mean(),
        "Unique Words": prepared_songs_dataset['Unique Word Counts'][prepared_songs_dataset['Year'] == years[i]].mean()
    }
    summary_dataset = summary_dataset.append(row, ignore_index=True)
summary_dataset["Year"] = summary_dataset['Year'].astype(int)

characteristics = prepared_songs_dataset.groupby('Year').count()

```

```{python, echo=FALSE, eval = FALSE, message=FALSE}
import pandas as pd
import numpy as np
import requests
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import HTML, display
from bs4 import BeautifulSoup
from nltk.tokenize import RegexpTokenizer
import lyricsgenius as genius
import sys
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string
from nltk.stem import PorterStemmer
from datetime import datetime
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
from collections import Counter
from os import path
from PIL import Image
import pickle
import json
from nltk.tokenize import RegexpTokenizer
from stop_words import get_stop_words
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models
import gensim
```

```{r, echo=TRUE, eval=FALSE, message=FALSE}
final_songs = pd.read_csv("final_songs.csv", index_col=0)

tokenizer = RegexpTokenizer(r'\w+')

# create English stop words list
en_stop = get_stop_words('en')

# Create p_stemmer of class PorterStemmer
p_stemmer = PorterStemmer()

doc_set = final_songs['Corpus'].tolist()

stops = ["love", "time", "day", "night", "girl", "baby", "babi", "like", "chorus", "verse", "bridge", "yeah", "whoa", "because", "come", "nigga", "thing"]

texts = []

# loop through document list
for i in doc_set:
    
    # clean and tokenize document string
    raw = i.lower()
    raw = re.sub(r'\b\w{1,3}\b', '', i)
    tokens = tokenizer.tokenize(raw)

    # remove stop words from tokens
    stopped_tokens = [i for i in tokens if not i in en_stop]

    stopped_tokens = [i for i in tokens if not i in stops]

    
    # # stem tokens
    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]
    stemmed_tokens = [i for i in stemmed_tokens if not i in stops]
    # stemmed_tokens = stopped_tokens
    # add tokens to list
    texts.append(stemmed_tokens)

# turn our tokenized documents into a id <-> term dictionary
dictionary = corpora.Dictionary(texts)
    
# convert tokenized documents into a document-term matrix
corpus = [dictionary.doc2bow(text) for text in texts]

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics= 9, id2word = dictionary, passes=20)

lda_results = [(0, '0.136*"heart" + 0.025*"dream" + 0.021*"fire" + 0.018*"woah" + 0.017*"kiss"'), (1, '0.095*"life" + 0.074*"thing" + 0.049*"world" + 0.027*"song" + 0.019*"people"'), (2, '0.104*"tonight" + 0.042*"dance" + 0.037*"party" + 0.024*"hand" + 0.021*"thing"'), (3, '0.058*"friend" + 0.050*"head" + 0.040*"woman" + 0.034*"lover" + 0.029*"lady", ladies'), (4, '0.095*"shit" + 0.086*"bitch" + 0.016*"bottom" + 0.015*"water" + 0.014*"pussies + 0.014"'), (5, '0.098*"money" + 0.067*"bodies" + 0.022*"type" + 0.016*"bitch" + 0.015*"cash"'), (6, '0.037*"name" + 0.025*"chance" + 0.023*"town" + 0.019*"rain" + 0.017*"tear"'), (7, '0.066*"gang" + 0.056*"taste" + 0.039*"girlfriend" + 0.028*"wrist" + 0.019*"chain"'), (8, '0.109*"thunder" + 0.040*"star" + 0.033*"murder" + 0.022*"ghost" + 0.020*"wish"')]


```


## Positive / Negative Sentiment Word Clouds By Decade

1970

```{r, echo=FALSE, message=FALSE}
library(reshape2)
library(wordcloud)


song_compcloud70 <- songtoken70 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

1980

```{r, echo=FALSE, message=FALSE}
song_tidy80 <- songtoken80 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

1990

```{r, echo=FALSE, message=FALSE}
song_tidy90 <- songtoken90 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

2000

```{r, echo=FALSE, message=FALSE}
song_tidy00 <- songtoken00 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

2010

```{r, echo=FALSE, message=FALSE}
song_tidy10 <- songtoken10 %>% 
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```



## Word Importance By Timeless Artists

Timeless artists, according to the data, included performers such as Taylor Swift, Michael Jackson, Elton John, Cher, Mariah Carey, etc.

```{r, echo=FALSE, message=FALSE}
timeless_artists <- songs %>%
  select(Artist, Year) %>%
  group_by(Year) %>%
  distinct(Artist) %>%
  ungroup() %>%
  group_by(Artist) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(25)

topartist_token <- songs %>%
  mutate(timeless = ifelse(Artist == "Madonna" | Artist == "Elton John" | Artist == "Mariah Carey" | Artist == "Taylor Swift" | Artist == "Chicago" | Artist == "Stevie Wonder" | Artist == "Kelly Clarkson" | Artist == "Michael Jackson" | Artist == "Rihanna" | Artist == "Aerosmith" | Artist == "Daryl Hall and John Oates" | Artist ==	"Drake"	| Artist == "Janet Jackson" | Artist ==	"Beyonce"	| Artist == "Bon Jovi" | Artist == "Celine Dion" | Artist == "Cher" | Artist == "Commodores" | Artist == "Eminem" | Artist == "Eric Clapton" | Artist == "Justin Timberlake" | Artist == "Maroon 5" | Artist == "Olivia Newton-John" | Artist =="Rod Stewart" | Artist =="Usher", "Top Artist", "Total"))

tfidf_words_timeless <- topartist_token %>%
  unnest_tokens(word, Lyrics) %>%
  distinct() %>%
  filter(nchar(word) > 3) %>%
  count(timeless, word, sort = TRUE) %>%
  bind_tf_idf(word, timeless, n) %>%
  arrange(desc(tf_idf))

top_tfidf_words_timeless <- tfidf_words_timeless %>% 
  group_by(timeless) %>% 
  slice(seq_len(10)) %>%
  ungroup() %>%
  arrange(timeless, tf_idf) %>%
  mutate(row = row_number())

top_tfidf_words_timeless %>%
  ggplot(aes(x = row, tf_idf, fill = timeless)) +
    geom_col(show.legend = NULL) +
    labs(x = NULL, y = "TF-IDF") + 
    ggtitle("Important Words using TF-IDF by Decade") +
    theme_lyrics() +  
    facet_wrap(~timeless, 
               ncol = 3, nrow = 2, 
               scales = "free") +
    scale_x_continuous(  # this handles replacement of row 
        breaks = top_tfidf_words_timeless$row, # notice need to reuse dataframe
        labels = top_tfidf_words_timeless$word) +
    coord_flip()

```



# CITATIONS

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2019). dplyr: A Grammar of Data Manipulation. R package version 0.8.3. https://CRAN.R-project.org/package=dplyr

Hadley Wickham and Lionel Henry (2019). tidyr: Tidy Messy Data. R package version 1.0.0.
https://CRAN.R-project.org/package=tidyr

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Yihui Xie (2019). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.24.

Gu, Z. (2014) circlize implements and enhances circular visualization in R. Bioinformatics.

Silge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” _JOSS_, *1*(3). doi: 10.21105/joss.00037 (URL: https://doi.org/10.21105/joss.00037), <URL:
http://dx.doi.org/10.21105/joss.00037>.

Emil Hvitfeldt (2019). textdata: Download and Load Various Text Datasets. R package version 0.3.0.
https://CRAN.R-project.org/package=textdata

Berger, Jonah, and Grant Packard. "Are atypical things more popular?." Psychological science 29.7 (2018): 1178-1184.

Tang, Farren. "Beginner's Guide to LDA Topic Modelling with R." <https://towardsdatascience.com/beginners-guide-to-lda-topic-modelling-with-r-e57a5a8e7a25>.

https://github.com/sharpie-007/dataAndMusic/blob/master/49%20Years%20of%20Music%20-%20Collection%20and%20Analysis.ipynb

https://www.billboard.com/articles/news/8427967/billboard-changes-streaming-weighting-hot-100-billboard-200

